{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11374507,"sourceType":"datasetVersion","datasetId":7120985}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nTop-Q1 Journal Standard: Refined Segmentation using U-Net++ for Cancerous ROI\n\nDataset structure:\n    dataset/\n        train/\n            benign/\n            malignant/\n        test/\n            benign/\n            malignant/\n\nDesired output:\n    The background is black, while the lesion (ROI) retains its original color from the input image.\n\nAuthor: [Your Name]\nDate: [Current Date]\n\"\"\"\n\nimport os\nimport cv2\nimport time\nimport copy\nimport random\nimport zipfile\nimport numpy as np\nfrom glob import glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n#############################################\n# 1. Reproducibility & Device Setup\n#############################################\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n#############################################\n# 2. Helper Functions: Pseudo Mask Generation & Image Conversion\n#############################################\ndef generate_pseudo_mask(pil_img):\n    \"\"\"\n    Generates a binary pseudo-mask using Otsu thresholding.\n    Then inverts it so that the lesion area is white (255) and background is black (0).\n    \"\"\"\n    gray = np.array(pil_img.convert(\"L\"))\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    _, mask = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    # Ensure binary\n    mask = (mask > 127).astype(np.uint8) * 255\n    # Force the lesion area to be white (255). If Otsu picks it as black, invert:\n    # This simple approach inverts the mask unconditionally, ensuring the \"dark\" region becomes white.\n    # If your dataset is reversed, you may want to add logic to detect which region is the lesion and invert if needed.\n    mask = 255 - mask\n    return Image.fromarray(mask, mode='L')\n\ndef random_horizontal_flip(image, mask, p=0.5):\n    if random.random() < p:\n        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n        mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n    return image, mask\n\nclass SegmentationTransforms:\n    \"\"\"\n    Paired augmentation for image and corresponding pseudo-mask.\n    \"\"\"\n    def __init__(self, flip_prob=0.5):\n        self.flip_prob = flip_prob\n\n    def __call__(self, image, mask):\n        image, mask = random_horizontal_flip(image, mask, self.flip_prob)\n        return image, mask\n\ndef to_tensor_image(pil_img):\n    \"\"\"\n    Converts a PIL RGB image to a torch.FloatTensor and normalizes to [0, 1].\n    \"\"\"\n    img = np.array(pil_img).astype(np.float32) / 255.0  # shape: (H, W, C)\n    img = np.transpose(img, (2, 0, 1))  # shape: (C, H, W)\n    return torch.from_numpy(img)\n\ndef to_tensor_mask(pil_mask):\n    \"\"\"\n    Converts a PIL grayscale mask to a torch.FloatTensor (1 x H x W) with values in [0, 1].\n    \"\"\"\n    mask = np.array(pil_mask).astype(np.float32) / 255.0\n    mask = np.expand_dims(mask, axis=0)\n    return torch.from_numpy(mask)\n\n#############################################\n# 3. Custom Dataset Class\n#############################################\nclass SkinSegmentationDataset(Dataset):\n    \"\"\"\n    Dataset for segmentation. Assumes images are stored in class folders\n    (e.g. train/benign, train/malignant).\n    A pseudoâ€“ground-truth mask is generated for each image.\n    \"\"\"\n    def __init__(self, root_dir, transform=None):\n        \"\"\"\n        root_dir: directory containing class subfolders.\n        transform: paired transformation (to both image and mask)\n        \"\"\"\n        super().__init__()\n        self.root_dir = root_dir\n        self.image_paths = []\n        self.labels = []\n        classes = os.listdir(root_dir)\n        self.classes = sorted(classes)\n        for cls in self.classes:\n            cls_folder = os.path.join(root_dir, cls)\n            for ext in ('*.png', '*.jpg', '*.jpeg'):\n                paths = glob(os.path.join(cls_folder, ext))\n                self.image_paths.extend(paths)\n                self.labels.extend([cls] * len(paths))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        mask = generate_pseudo_mask(image)\n        if self.transform:\n            image, mask = self.transform(image, mask)\n        image = to_tensor_image(image)\n        mask = to_tensor_mask(mask)\n        return image, mask, img_path\n\n#############################################\n# 4. U-Net++ Model Definition\n#############################################\ndef conv_block(in_channels, out_channels):\n    \"\"\"\n    Basic two-layer convolution block: Conv->BN->ReLU -> Conv->BN->ReLU\n    \"\"\"\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True)\n    )\n\ndef up_conv(in_channels, out_channels):\n    \"\"\"\n    Upsampling followed by a convolution block.\n    \"\"\"\n    return nn.Sequential(\n        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU(inplace=True)\n    )\n\nclass UNetPlusPlus(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1):\n        \"\"\"\n        A simplified implementation of U-Net++ with 3 nested levels.\n        \"\"\"\n        super(UNetPlusPlus, self).__init__()\n        nb_filter = [32, 64, 128, 256, 512]\n\n        # Encoder\n        self.conv0_0 = conv_block(in_channels, nb_filter[0])\n        self.pool0   = nn.MaxPool2d(2)\n\n        self.conv1_0 = conv_block(nb_filter[0], nb_filter[1])\n        self.pool1   = nn.MaxPool2d(2)\n\n        self.conv2_0 = conv_block(nb_filter[1], nb_filter[2])\n        self.pool2   = nn.MaxPool2d(2)\n\n        self.conv3_0 = conv_block(nb_filter[2], nb_filter[3])\n        self.pool3   = nn.MaxPool2d(2)\n\n        self.conv4_0 = conv_block(nb_filter[3], nb_filter[4])\n\n        # Decoder (Nested)\n        self.up0_1 = up_conv(nb_filter[1], nb_filter[0])\n        self.conv0_1 = conv_block(nb_filter[0]*2, nb_filter[0])\n\n        self.up1_1 = up_conv(nb_filter[2], nb_filter[1])\n        self.conv1_1 = conv_block(nb_filter[1]*2, nb_filter[1])\n\n        self.up2_1 = up_conv(nb_filter[3], nb_filter[2])\n        self.conv2_1 = conv_block(nb_filter[2]*2, nb_filter[2])\n\n        self.up3_1 = up_conv(nb_filter[4], nb_filter[3])\n        self.conv3_1 = conv_block(nb_filter[3]*2, nb_filter[3])\n\n        self.up0_2 = up_conv(nb_filter[1], nb_filter[0])\n        self.conv0_2 = conv_block(nb_filter[0]*3, nb_filter[0])\n\n        self.up1_2 = up_conv(nb_filter[2], nb_filter[1])\n        self.conv1_2 = conv_block(nb_filter[1]*3, nb_filter[1])\n\n        self.up0_3 = up_conv(nb_filter[1], nb_filter[0])\n        self.conv0_3 = conv_block(nb_filter[0]*4, nb_filter[0])\n\n        self.final = nn.Conv2d(nb_filter[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Encoder\n        x0_0 = self.conv0_0(x)               \n        x1_0 = self.conv1_0(self.pool0(x0_0)) \n        x2_0 = self.conv2_0(self.pool1(x1_0)) \n        x3_0 = self.conv3_0(self.pool2(x2_0)) \n        x4_0 = self.conv4_0(self.pool3(x3_0)) \n\n        # 1st nested level\n        x0_1 = self.conv0_1(torch.cat([x0_0, self.up0_1(x1_0)], 1))\n        x1_1 = self.conv1_1(torch.cat([x1_0, self.up1_1(x2_0)], 1))\n        x2_1 = self.conv2_1(torch.cat([x2_0, self.up2_1(x3_0)], 1))\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.up3_1(x4_0)], 1))\n\n        # 2nd nested level\n        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up0_2(x1_1)], 1))\n        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up1_2(x2_1)], 1))\n\n        # 3rd nested level\n        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up0_3(x1_2)], 1))\n\n        output = self.final(x0_3)\n        return output\n\n#############################################\n# 5. Loss Functions: BCE + Dice Loss\n#############################################\ncriterion_bce = nn.BCEWithLogitsLoss()\n\ndef dice_loss(pred, target, smooth=1.):\n    pred = torch.sigmoid(pred)\n    pred = pred.view(-1)\n    target = target.view(-1)\n    intersection = (pred * target).sum()\n    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n    return 1 - dice\n\ndef combined_loss(pred, target):\n    return criterion_bce(pred, target) + dice_loss(pred, target)\n\n#############################################\n# 6. Training Function for Segmentation\n#############################################\ndef train_segmentation(model, dataloader, optimizer, num_epochs=25):\n    model.train()\n    history = []\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        start_time = time.time()\n        for imgs, masks, _ in dataloader:\n            imgs = imgs.to(device)\n            masks = masks.to(device)\n            optimizer.zero_grad()\n            outputs = model(imgs)\n            loss = combined_loss(outputs, masks)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * imgs.size(0)\n\n        epoch_loss /= len(dataloader.dataset)\n        elapsed = time.time() - start_time\n        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f} - Time: {elapsed:.1f}s\")\n        history.append(epoch_loss)\n    return history\n\n#############################################\n# 7. Inference & Saving Segmented Images\n#############################################\ndef inference_and_save(model, dataset_root, output_root):\n    \"\"\"\n    For every image in dataset_root (organized in subfolders), run inference to compute a\n    segmentation mask. Then create an output image where:\n      - Background pixels (mask==0) are set to black.\n      - ROI pixels (mask==1) retain their original color.\n    The output is saved preserving the original folder structure.\n    \"\"\"\n    model.eval()\n    os.makedirs(output_root, exist_ok=True)\n    for cls in os.listdir(dataset_root):\n        cls_input_folder = os.path.join(dataset_root, cls)\n        cls_output_folder = os.path.join(output_root, cls)\n        os.makedirs(cls_output_folder, exist_ok=True)\n        for ext in ('*.png', '*.jpg', '*.jpeg'):\n            for img_path in glob(os.path.join(cls_input_folder, ext)):\n                # Load and prepare\n                orig_img = Image.open(img_path).convert(\"RGB\")\n                img_tensor = to_tensor_image(orig_img).unsqueeze(0).to(device)\n                with torch.no_grad():\n                    output = model(img_tensor)\n                    prob = torch.sigmoid(output)\n                    mask = (prob > 0.5).float()  # threshold to binary\n                # Convert mask to numpy\n                mask_np = mask.squeeze().cpu().numpy().astype(np.uint8)\n                # Multiply each pixel in the original image by the mask => black background\n                orig_np = np.array(orig_img)\n                seg_np = orig_np * np.expand_dims(mask_np, axis=2)\n                seg_pil = Image.fromarray(seg_np.astype(np.uint8))\n                base_name = os.path.basename(img_path)\n                seg_pil.save(os.path.join(cls_output_folder, base_name))\n\n    print(f\"Segmented images saved in: {output_root}\")\n\n#############################################\n# 8. Zip the Output Directory\n#############################################\ndef zip_directory(folder_path, zip_path):\n    \"\"\"\n    Compress the folder at folder_path into a zip file saved at zip_path.\n    \"\"\"\n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, _, files in os.walk(folder_path):\n            for file in files:\n                abs_path = os.path.join(root, file)\n                rel_path = os.path.relpath(abs_path, folder_path)\n                zipf.write(abs_path, arcname=rel_path)\n    print(f\"Output zipped to: {zip_path}\")\n\n#############################################\n# 9. Main Script\n#############################################\nif __name__ == \"__main__\":\n    # (1) Update dataset_dir with your data path\n    dataset_dir = \"/kaggle/input/benign-malignant-original/archive\"   # has \"train/\" and \"test/\" subfolders\n    train_dir   = os.path.join(dataset_dir, \"train\")\n    test_dir    = os.path.join(dataset_dir, \"test\")\n\n    # (2) Output directory for segmented images\n    output_segmentation_dir = \"/kaggle/working/segmented\"\n\n    # (3) Create dataset & DataLoader for training\n    seg_transforms = SegmentationTransforms(flip_prob=0.5)\n    train_dataset = SkinSegmentationDataset(train_dir, transform=seg_transforms)\n    train_loader  = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n\n    # (4) Initialize U-Net++ model & optimizer\n    model = UNetPlusPlus(in_channels=3, out_channels=1).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n    # (5) Train the segmentation model\n    num_epochs = 25\n    print(\"Starting training of U-Net++ for segmentation...\")\n    train_history = train_segmentation(model, train_loader, optimizer, num_epochs=num_epochs)\n\n    # (6) (Optional) Save the trained model weights\n    torch.save(model.state_dict(), \"/kaggle/working/unetpp_weights.pth\")\n    print(\"Model training completed and weights saved.\")\n\n    # (7) Inference on train & test sets => black background, lesion retains color\n    segmented_train_out = os.path.join(output_segmentation_dir, \"train\")\n    segmented_test_out  = os.path.join(output_segmentation_dir, \"test\")\n\n    print(\"Running inference on training images...\")\n    inference_and_save(model, train_dir, segmented_train_out)\n    print(\"Running inference on testing images...\")\n    inference_and_save(model, test_dir, segmented_test_out)\n\n    # (8) Zip the entire segmented output folder\n    zip_path = \"/kaggle/working/segmented_output.zip\"\n    zip_directory(output_segmentation_dir, zip_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:36:20.884780Z","iopub.execute_input":"2025-04-12T14:36:20.885457Z","iopub.status.idle":"2025-04-12T15:23:34.288396Z","shell.execute_reply.started":"2025-04-12T14:36:20.885429Z","shell.execute_reply":"2025-04-12T15:23:34.287328Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda:0\nStarting training of U-Net++ for segmentation...\nEpoch 1/25 - Loss: 0.6745 - Time: 101.8s\nEpoch 2/25 - Loss: 0.4987 - Time: 110.7s\nEpoch 3/25 - Loss: 0.4130 - Time: 110.7s\nEpoch 4/25 - Loss: 0.3674 - Time: 110.8s\nEpoch 5/25 - Loss: 0.3461 - Time: 110.9s\nEpoch 6/25 - Loss: 0.3345 - Time: 111.0s\nEpoch 7/25 - Loss: 0.3190 - Time: 111.0s\nEpoch 8/25 - Loss: 0.3056 - Time: 110.9s\nEpoch 9/25 - Loss: 0.3043 - Time: 110.7s\nEpoch 10/25 - Loss: 0.2894 - Time: 111.1s\nEpoch 11/25 - Loss: 0.2865 - Time: 111.4s\nEpoch 12/25 - Loss: 0.2808 - Time: 111.6s\nEpoch 13/25 - Loss: 0.2797 - Time: 110.7s\nEpoch 14/25 - Loss: 0.2762 - Time: 111.3s\nEpoch 15/25 - Loss: 0.2708 - Time: 111.7s\nEpoch 16/25 - Loss: 0.2732 - Time: 111.5s\nEpoch 17/25 - Loss: 0.2658 - Time: 111.8s\nEpoch 18/25 - Loss: 0.2566 - Time: 111.7s\nEpoch 19/25 - Loss: 0.2534 - Time: 111.9s\nEpoch 20/25 - Loss: 0.2601 - Time: 111.6s\nEpoch 21/25 - Loss: 0.2554 - Time: 111.6s\nEpoch 22/25 - Loss: 0.2483 - Time: 111.6s\nEpoch 23/25 - Loss: 0.2516 - Time: 111.0s\nEpoch 24/25 - Loss: 0.2561 - Time: 110.8s\nEpoch 25/25 - Loss: 0.2442 - Time: 111.1s\nModel training completed and weights saved.\nRunning inference on training images...\nSegmented images saved in: /kaggle/working/segmented/train\nRunning inference on testing images...\nSegmented images saved in: /kaggle/working/segmented/test\nOutput zipped to: /kaggle/working/segmented_output.zip\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport shutil\n\ndef clean_kaggle_output_dir(dir_path=\"/kaggle/working\"):                                                                                       \n    \"\"\"\n    Removes all files and directories in the specified Kaggle output directory.\n\n    Parameters:\n        dir_path (str): Path to the directory to clean.\n    \"\"\"\n    # Confirm the directory exists\n    if not os.path.exists(dir_path):\n        print(f\"Directory {dir_path} does not exist. Creating it.\")\n        os.makedirs(dir_path)\n        return\n\n    for filename in os.listdir(dir_path):\n        file_path = os.path.join(dir_path, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)  # remove the file or link\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)  # remove the directory and its content\n        except Exception as e:\n            print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n    print(f\"Cleaned the Kaggle output directory: {dir_path}\")\n\nif __name__ == \"__main__\":\n    # Clean the default Kaggle working directory\n    clean_kaggle_output_dir(\"/kaggle/working\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-12T14:36:10.932646Z","iopub.execute_input":"2025-04-12T14:36:10.933396Z","iopub.status.idle":"2025-04-12T14:36:10.942725Z","shell.execute_reply.started":"2025-04-12T14:36:10.933370Z","shell.execute_reply":"2025-04-12T14:36:10.942115Z"}},"outputs":[{"name":"stdout","text":"Cleaned the Kaggle output directory: /kaggle/working\n","output_type":"stream"}],"execution_count":1}]}